import logging
from sqlalchemy import text
from app.db.session import engine
from app.db.base import Base

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_migrations():
    """
    Run ad-hoc migrations to fix schema discrepancies.
    Using connection.commit() instead of raw SQL COMMIT to avoid transaction warnings.
    """
    try:
        with engine.connect() as conn:
            # 1. Verification of missing columns in existing tables
            migrations = [
                ("items", "category", "VARCHAR(64)"),
                ("items", "source_sample_id", "UUID REFERENCES items(id)"),
                ("items", "attribute_id", "UUID REFERENCES attributes(id)"),
                ("work_orders", "location_id", "UUID REFERENCES locations(id)"),
                ("work_orders", "source_location_id", "UUID REFERENCES locations(id)"),
                ("work_orders", "completed_at", "TIMESTAMP WITHOUT TIME ZONE"),
                ("bom_lines", "source_location_id", "UUID REFERENCES locations(id)"),
                ("bom_lines", "is_percentage", "BOOLEAN DEFAULT FALSE"),
                ("boms", "tolerance_percentage", "NUMERIC(5,2) DEFAULT 0.0"),
                ("purchase_orders", "target_location_id", "UUID REFERENCES locations(id)"),
                ("users", "hashed_password", "VARCHAR(255)"),
                ("users", "allowed_categories", "JSON"),
            ]

            for table, col, col_type in migrations:
                try:
                    # Check if column exists
                    res = conn.execute(text(f"SELECT column_name FROM information_schema.columns WHERE table_name='{table}' AND column_name='{col}'"))
                    if not res.fetchone():
                        logger.info(f"Migration: Adding {col} to {table}")
                        conn.execute(text(f"ALTER TABLE {table} ADD COLUMN {col} {col_type}"))
                        conn.commit()
                except Exception as e:
                    logger.warning(f"Migration for {table}.{col} failed: {e}")

            # 1b. Verification of indexes for high-volume data
            index_migrations = [
                ("idx_items_category", "items", "category"),
                ("idx_bom_lines_item_id", "bom_lines", "item_id"),
                ("idx_work_orders_item_id", "work_orders", "item_id"),
                ("idx_audit_logs_entity_type", "audit_logs", "entity_type"),
                ("idx_audit_logs_entity_id", "audit_logs", "entity_id"),
                ("idx_audit_logs_timestamp", "audit_logs", "timestamp"),
                ("idx_sample_requests_so_id", "sample_requests", "sales_order_id"),
                ("idx_sample_requests_base_id", "sample_requests", "base_item_id"),
            ]

            for idx_name, table, col in index_migrations:
                try:
                    logger.info(f"Migration: Ensuring index {idx_name} on {table}({col})")
                    conn.execute(text(f"CREATE INDEX IF NOT EXISTS {idx_name} ON {table} ({col})"))
                    conn.commit()
                except Exception as e:
                    logger.warning(f"Index migration {idx_name} failed: {e}")

            # 1c. Advanced Search Optimization (Trigrams)
            try:
                logger.info("Migration: Enabling pg_trgm extension")
                conn.execute(text("CREATE EXTENSION IF NOT EXISTS pg_trgm"))
                conn.commit()
                
                # GIN indexes for fuzzy search on large text volumes
                conn.execute(text("CREATE INDEX IF NOT EXISTS idx_items_code_trgm ON items USING gin (code gin_trgm_ops)"))
                conn.execute(text("CREATE INDEX IF NOT EXISTS idx_items_name_trgm ON items USING gin (name gin_trgm_ops)"))
                conn.commit()
                logger.info("Migration: Created GIN trigram indexes for high-speed search")
            except Exception as e:
                logger.warning(f"Trigram index creation failed: {e}")

            # 2. Data Migration: Move single attribute_id/attribute_value_id to secondary tables if data exists
            # These are the many-to-many migrations
            move_data = [
                ("items", "attribute_id", "item_attributes", "item_id", "attribute_id"),
                ("stock_ledger", "attribute_value_id", "stock_ledger_values", "stock_ledger_id", "attribute_value_id"),
                ("boms", "attribute_value_id", "bom_values", "bom_id", "attribute_value_id"),
                ("bom_lines", "attribute_value_id", "bom_line_values", "bom_line_id", "attribute_value_id"),
                ("work_orders", "attribute_value_id", "work_order_values", "work_order_id", "attribute_value_id")
            ]

            for src_table, src_col, target_table, target_id_col, target_val_col in move_data:
                try:
                    # Check if src_col exists before attempting migration
                    res = conn.execute(text(f"SELECT column_name FROM information_schema.columns WHERE table_name='{src_table}' AND column_name='{src_col}'"))
                    if res.fetchone():
                        conn.execute(text(f"""
                            INSERT INTO {target_table} ({target_id_col}, {target_val_col}) 
                            SELECT id, {src_col} FROM {src_table} 
                            WHERE {src_col} IS NOT NULL 
                            ON CONFLICT DO NOTHING
                        """))
                        conn.commit()
                        logger.info(f"Migration: Moved {src_col} data to {target_table}")
                except Exception as e:
                    pass

            # 3. Verify Routing Tables (WorkCenter, Operation)
            try:
                # Just a simple check to ensure they exist (create_all should have handled it)
                conn.execute(text("SELECT 1 FROM work_centers LIMIT 1"))
                conn.execute(text("SELECT 1 FROM operations LIMIT 1"))
                conn.execute(text("SELECT 1 FROM bom_operations LIMIT 1"))
                conn.execute(text("SELECT 1 FROM sales_orders LIMIT 1"))
                conn.execute(text("SELECT 1 FROM sample_requests LIMIT 1"))
                conn.execute(text("SELECT 1 FROM audit_logs LIMIT 1"))
                conn.execute(text("SELECT 1 FROM partners LIMIT 1"))
                conn.execute(text("SELECT 1 FROM purchase_orders LIMIT 1"))
                logger.info("Migration: Verified routing and partner tables")
            except Exception as e:
                pass

    except Exception as e:
        logger.error(f"Migration engine failed: {e}")

from app.models.category import Category
from app.models.auth import Permission, Role, User
from app.models.uom import UOM
from app.core.security import get_password_hash

def seed_categories(db):
    try:
        if db.query(Category).count() == 0:
            defaults = ["Raw Material", "WIP", "Finished Goods", "Sample", "Consumable"]
            for name in defaults:
                db.add(Category(name=name))
            db.commit()
            logger.info("Seeded default categories")
    except Exception as e:
        logger.warning(f"Category seeding skipped: {e}")

def seed_uoms(db):
    try:
        if db.query(UOM).count() == 0:
            defaults = ["pcs", "kg", "m", "l", "box", "roll"]
            for name in defaults:
                db.add(UOM(name=name))
            db.commit()
            logger.info("Seeded default UOMs")
    except Exception as e:
        logger.warning(f"UOM seeding skipped: {e}")

def seed_rbac(db):
    try:
        # 1. Define Permissions
        perms_data = [
            ("inventory.manage", "Manage Items, Attributes, Categories"),
            ("inventory.delete", "Delete Inventory Data"),
            ("locations.manage", "Manage Locations"),
            ("manufacturing.manage", "Manage BOMs and Routing"),
            ("work_order.manage", "Create and Update Work Orders"),
            ("stock.entry", "Record Stock Movements"),
            ("reports.view", "View Reports"),
            ("admin.access", "Full System Access"),
        ]
        
        db_perms = {}
        for code, desc in perms_data:
            perm = db.query(Permission).filter(Permission.code == code).first()
            if not perm:
                perm = Permission(code=code, description=desc)
                db.add(perm)
                db.commit()
                db.refresh(perm)
            db_perms[code] = perm
            
        # 2. Define Roles
        roles_data = {
            "Administrator": ["admin.access", "inventory.manage", "inventory.delete", "locations.manage", "manufacturing.manage", "work_order.manage", "stock.entry", "reports.view"],
            "Store Manager": ["inventory.manage", "stock.entry", "reports.view"],
            "Production Manager": ["manufacturing.manage", "work_order.manage", "reports.view"],
            "Operator": ["work_order.manage"]
        }
        
        for role_name, perm_codes in roles_data.items():
            role = db.query(Role).filter(Role.name == role_name).first()
            if not role:
                role = Role(name=role_name)
                db.add(role)
                db.commit()
                db.refresh(role)
            
            # Assign permissions
            current_perms = role.permissions
            for code in perm_codes:
                if db_perms[code] not in current_perms:
                    role.permissions.append(db_perms[code])
            db.commit()

        # 3. Seed Users (Simulated)
        users_data = [
            ("admin", "System Admin", "Administrator"),
            ("store_mgr", "Budi Store", "Store Manager"),
            ("prod_mgr", "Siti Production", "Production Manager"),
            ("operator", "Joko Worker", "Operator"),
        ]

        for uname, fname, rname in users_data:
            user = db.query(User).filter(User.username == uname).first()
            if not user:
                role = db.query(Role).filter(Role.name == rname).first()
                user = User(
                    username=uname, 
                    full_name=fname, 
                    role_id=role.id,
                    hashed_password=get_password_hash("password") # Default password
                )
                db.add(user)
                db.commit()
            else:
                # Ensure password is set correctly (Reset for dev/demo)
                # In production, remove this else block
                user.hashed_password = get_password_hash("password")
                db.commit()

        logger.info("Seeded RBAC (Roles, Permissions, Users)")

    except Exception as e:
        logger.error(f"RBAC seeding failed: {e}")

from app.models.stock_ledger import StockLedger
from app.models.stock_balance import StockBalance
from app.models.attribute import AttributeValue
from app.services import stock_service

def sync_stock_balances(db):
    """
    Synchronizes the pre-calculated stock_balances table with the existing stock_ledger.
    Forces string keys to ensure robust grouping.
    """
    try:
        logger.info("Synchronizing Stock Balances from Ledger...")
        
        # 1. Clear existing summary
        db.execute(text("TRUNCATE stock_balances, stock_balance_values CASCADE"))
        db.commit()

        # 2. Aggregate all ledger entries in memory
        entries = db.query(StockLedger).all()
        aggregated = {} # key: "item_id:loc_id:v_key" -> {qty, attr_ids, raw_item_id, raw_loc_id}

        for e in entries:
            attr_ids = [str(v.id) for v in e.attribute_values]
            v_key = stock_service._generate_variant_key(attr_ids)
            # Force to string to ensure dictionary key uniqueness works across different object instances
            s_key = f"{str(e.item_id)}:{str(e.location_id)}:{v_key}"

            if s_key not in aggregated:
                aggregated[s_key] = {
                    "qty": 0.0, 
                    "attr_ids": attr_ids,
                    "item_id": e.item_id,
                    "location_id": e.location_id,
                    "v_key": v_key
                }
            
            aggregated[s_key]["qty"] += float(e.qty_change)

        logger.info(f"Aggregated {len(entries)} ledger entries into {len(aggregated)} unique balance records.")

        # 3. Create balance records
        for s_key, data in aggregated.items():
            balance = StockBalance(
                item_id=data["item_id"],
                location_id=data["location_id"],
                variant_key=data["v_key"],
                qty=data["qty"]
            )
            if data["attr_ids"]:
                vals = db.query(AttributeValue).filter(AttributeValue.id.in_(data["attr_ids"])).all()
                balance.attribute_values = vals
            db.add(balance)
        
        db.commit()
        logger.info("Stock synchronization successfully committed.")
    except Exception as e:
        logger.error(f"Stock synchronization failed: {e}")
        db.rollback()

def init_db() -> None:
    logger.info("Initializing Database...")
    # 1. Create all tables (including association tables registered in base.py)
    Base.metadata.create_all(bind=engine)
    
    # 2. Run ad-hoc column migrations
    run_migrations()
    
    # 3. Seed and Sync data
    from app.db.session import SessionLocal
    db = SessionLocal()
    try:
        seed_categories(db)
        seed_uoms(db)
        seed_rbac(db)
        sync_stock_balances(db) # Perform sync
    finally:
        db.close()
        
    logger.info("Database initialization complete.")

if __name__ == "__main__":
    init_db()